### /src/rag
- `manager.py` - Main RAG orchestrator: loads files from `/memory`, chunks text, generates embeddings, retrieves relevant chunks
- `embeddings.py` - Chunking logic, embedding model management, and cosine similarity calculations
- `embeddings_cache.py` - Cache embeddings to avoid regenerating for unchanged files

## RAG Process

1. **Initialization** (on chat start):
   - Scan `/memory` directory for `.md` and `.txt` files
   - Load embedding cache from disk (`.embeddings_cache.pkl`)
   - For each file:
     - Check if file is cached and unchanged (timestamp-based)
     - If cached: load chunks and embeddings from cache (fast!)
     - If new/changed: chunk text (~800 chars/chunk) and queue for embedding
   - Generate embeddings only for new/changed files (batch processing)
   - Update cache with new embeddings
   - Save cache to disk
   - Smart chunking respects natural boundaries (paragraphs, lines, sentences, word breaks)

2. **Query-Time Retrieval** (each user message):
   - Generate embedding for user's query
   - Calculate cosine similarity between query and all chunk embeddings
   - Select top-k most similar chunks (ranked by relevance)
   - Use tokenizer for precise token counting
   - Fit retrieved chunks within RAG token budget (25% of context window)

3. **Prompt Construction** (in `context_manager.py`):
   - System prompt (always included, ~100-500 tokens)
   - RAG context (retrieved chunks, up to 25% of context window)
   - Chat history (fills remaining ~70%, pruned as needed)

4. **Model Generation**:
   - Model receives enriched prompt with relevant knowledge base context
   - Generates informed response using both chat history and RAG context
   - RAG context is ephemeral (not saved to chat history, re-retrieved each query)